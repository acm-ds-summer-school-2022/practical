{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ATIS RNN + SVM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Intent Classification and Slot Filling"
      ],
      "metadata": {
        "id": "RuepZts2t3rX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cloning the repository"
      ],
      "metadata": {
        "id": "4Kl7ZW4RnVZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/DSKSD/RNN-for-Joint-NLU.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nddVSyCNOmq0",
        "outputId": "518ec878-5608-460a-8cb5-3b00caacbf8c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'RNN-for-Joint-NLU'...\n",
            "remote: Enumerating objects: 57, done.\u001b[K\n",
            "remote: Total 57 (delta 0), reused 0 (delta 0), pack-reused 57\u001b[K\n",
            "Unpacking objects: 100% (57/57), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import pickle\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "lR5svXoOt6IW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/RNN-for-Joint-NLU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2G5lemDe_SB",
        "outputId": "40456072-5d6c-4939-ad85-498836a5ee92"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/RNN-for-Joint-NLU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbOQRo_HfDkK",
        "outputId": "568506af-d75a-4e4f-baa1-5da2b44a686d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/RNN-for-Joint-NLU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading the dataset"
      ],
      "metadata": {
        "id": "HLLHSTGBnZb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions:\n",
        "1. Download the data (*atis-2.train.w-intent.iob* and *atis-2.dev.w-intent.iob*) from [this repo](https://github.com/yvchen/JointSLU/tree/master/data) or from the repo.\n",
        "2. Move data (*atis-2.train.w-intent.iob* and *atis-2.dev.w-intent.iob*) to the cloned repository. Such as -> `/content/RNN-for-Joint-NLU/atis-2.train.w-intent.iob`and `/content/RNN-for-Joint-NLU/atis-2.dev.w-intent.iob`.\n",
        "\n",
        "Note:\n",
        "1. \"The `IOB` format (short for inside, outside, beginning) is a common tagging format for tagging tokens in a chunking task in computational linguistics (ex. named-entity recognition).\" Source: [Wikipedia](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging\\)#:~:text=The%20IOB%20format%20(short%20for,named%2Dentity%20recognition).))"
      ],
      "metadata": {
        "id": "PEZwmQ3Qk6KO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-processing and encodings"
      ],
      "metadata": {
        "id": "_UtX1E3IncHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "USE_CUDA"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwg6wWFMt6ab",
        "outputId": "cfe8905a-1210-4130-84ae-da2239f6eb5d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = list(map(lambda w: to_ix[w] if w in to_ix.keys() else to_ix[\"<UNK>\"], seq))\n",
        "    tensor = Variable(torch.LongTensor(idxs)).cuda() if USE_CUDA else Variable(torch.LongTensor(idxs))\n",
        "    return tensor\n",
        "\n",
        "\n",
        "flatten = lambda l: [item for sublist in l for item in sublist]"
      ],
      "metadata": {
        "id": "q1wxcQ9At8iL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = open(\"/content/RNN-for-Joint-NLU/atis-2.train.w-intent.iob\",\"r\").readlines()\n",
        "train = [t[:-1] for t in train]\n",
        "train = [[t.split(\"\\t\")[0].split(\" \"),t.split(\"\\t\")[1].split(\" \")[:-1],t.split(\"\\t\")[1].split(\" \")[-1]] for t in train]\n",
        "train = [[t[0][1:-1],t[1][1:],t[2]] for t in train]\n",
        "train_copy = train"
      ],
      "metadata": {
        "id": "2lRplF3it-Xx"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_in,seq_out, intent = list(zip(*train))\n",
        "vocab = set(flatten(seq_in))\n",
        "slot_tag = set(flatten(seq_out))\n",
        "intent_tag = set(intent)\n",
        "LENGTH=50\n",
        "sin=[]\n",
        "sout=[]"
      ],
      "metadata": {
        "id": "JDUErQO1BWe4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(seq_in)):\n",
        "    temp = seq_in[i]\n",
        "    if len(temp)<LENGTH:\n",
        "        temp.append('<EOS>')\n",
        "        while len(temp)<LENGTH:\n",
        "            temp.append('<PAD>')\n",
        "    else:\n",
        "        temp = temp[:LENGTH]\n",
        "        temp[-1]='<EOS>'\n",
        "    sin.append(temp)\n",
        "    \n",
        "    temp = seq_out[i]\n",
        "    if len(temp)<LENGTH:\n",
        "        while len(temp)<LENGTH:\n",
        "            temp.append('<PAD>')\n",
        "    else:\n",
        "        temp = temp[:LENGTH]\n",
        "        temp[-1]='<EOS>'\n",
        "    sout.append(temp)\n",
        "word2index = {'<PAD>': 0, '<UNK>':1,'<SOS>':2,'<EOS>':3}\n",
        "for token in vocab:\n",
        "    if token not in word2index.keys():\n",
        "        word2index[token]=len(word2index)\n",
        "\n",
        "index2word = {v:k for k,v in word2index.items()}\n",
        "\n",
        "tag2index = {'<PAD>' : 0}\n",
        "for tag in slot_tag:\n",
        "    if tag not in tag2index.keys():\n",
        "        tag2index[tag] = len(tag2index)\n",
        "index2tag = {v:k for k,v in tag2index.items()}\n",
        "\n",
        "intent2index={}\n",
        "for ii in intent_tag:\n",
        "    if ii not in intent2index.keys():\n",
        "        intent2index[ii] = len(intent2index)\n",
        "index2intent = {v:k for k,v in intent2index.items()}\n",
        "train = list(zip(sin,sout,intent))\n",
        "train[0][2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "5Q7gnNuWBbHl",
        "outputId": "69050a48-63b0-4998-da03-0b0f91730b65"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'atis_flight'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=[]\n",
        "\n",
        "for tr in train:\n",
        "    \n",
        "    temp = prepare_sequence(tr[0],word2index)\n",
        "    temp = temp.view(1,-1)\n",
        "    \n",
        "    temp2 = prepare_sequence(tr[1],tag2index)\n",
        "    temp2 = temp2.view(1,-1)\n",
        "    \n",
        "    temp3 = Variable(torch.LongTensor([intent2index[tr[2]]])).cuda() if USE_CUDA else Variable(torch.LongTensor([intent2index[tr[2]]]))\n",
        "    \n",
        "    train_data.append((temp,temp2,temp3))\n",
        "def getBatch(batch_size,train_data):\n",
        "    random.shuffle(train_data)\n",
        "    sindex=0\n",
        "    eindex=batch_size\n",
        "    while eindex < len(train_data):\n",
        "        batch = train_data[sindex:eindex]\n",
        "        temp = eindex\n",
        "        eindex = eindex+batch_size\n",
        "        sindex = temp\n",
        "        \n",
        "        yield batch"
      ],
      "metadata": {
        "id": "itxUZvEIBd3j"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Black-box model (RNN with attention)"
      ],
      "metadata": {
        "id": "az1jieb6BhLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size,embedding_size, hidden_size,batch_size=16 ,n_layers=1):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        self.input_size = input_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        self.batch_size=batch_size\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers, batch_first=True,bidirectional=True)\n",
        "    \n",
        "    def init_weights(self):\n",
        "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "    \n",
        "    def init_hidden(self,input):\n",
        "        hidden = Variable(torch.zeros(self.n_layers*2, input.size(0), self.hidden_size)).cuda() if USE_CUDA else Variable(torch.zeros(self.n_layers*2, input.size(0), self.hidden_size))\n",
        "        context = Variable(torch.zeros(self.n_layers*2, input.size(0), self.hidden_size)).cuda() if USE_CUDA else Variable(torch.zeros(self.n_layers*2, input.size(0), self.hidden_size))\n",
        "        return (hidden,context)\n",
        "     \n",
        "    def forward(self, input,input_masking):\n",
        "        self.hidden = self.init_hidden(input)\n",
        "        \n",
        "        embedded = self.embedding(input)\n",
        "        output, self.hidden = self.lstm(embedded, self.hidden)\n",
        "        \n",
        "        real_context=[]\n",
        "        \n",
        "        for i,o in enumerate(output): \n",
        "            real_length = input_masking[i].data.tolist().count(0) \n",
        "            real_context.append(o[real_length-1])\n",
        "            \n",
        "        return output, torch.cat(real_context).view(input.size(0),-1).unsqueeze(1)\n",
        "class Decoder(nn.Module):\n",
        "    \n",
        "    def __init__(self,slot_size,intent_size,embedding_size,hidden_size,batch_size=16,n_layers=1,dropout_p=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        self.hidden_size = hidden_size\n",
        "        self.slot_size = slot_size\n",
        "        self.intent_size = intent_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout_p = dropout_p\n",
        "        self.embedding_size = embedding_size\n",
        "        self.batch_size = batch_size\n",
        "        self.embedding = nn.Embedding(self.slot_size, self.embedding_size)\n",
        "        self.lstm = nn.LSTM(self.embedding_size+self.hidden_size*2, self.hidden_size, self.n_layers, batch_first=True)\n",
        "        self.attn = nn.Linear(self.hidden_size,self.hidden_size) \n",
        "        self.slot_out = nn.Linear(self.hidden_size*2, self.slot_size)\n",
        "        self.intent_out = nn.Linear(self.hidden_size*2,self.intent_size)\n",
        "    \n",
        "    def init_weights(self):\n",
        "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "    \n",
        "    def Attention(self, hidden, encoder_outputs, encoder_maskings):\n",
        "        \"\"\"\n",
        "        hidden : 1,B,D\n",
        "        encoder_outputs : B,T,D\n",
        "        encoder_maskings : B,T # ByteTensor\n",
        "        \"\"\"\n",
        "        \n",
        "        hidden = hidden.squeeze(0).unsqueeze(2)  \n",
        "        \n",
        "        batch_size = encoder_outputs.size(0) \n",
        "        max_len = encoder_outputs.size(1) \n",
        "        energies = self.attn(encoder_outputs.contiguous().view(batch_size*max_len,-1))\n",
        "        energies = energies.view(batch_size,max_len,-1) \n",
        "        attn_energies = energies.bmm(hidden).transpose(1,2)\n",
        "        attn_energies = attn_energies.squeeze(1).masked_fill(encoder_maskings,-1e12)\n",
        "        \n",
        "        alpha = F.softmax(attn_energies) \n",
        "        alpha = alpha.unsqueeze(1) \n",
        "        context = alpha.bmm(encoder_outputs) \n",
        "        \n",
        "        return context \n",
        "    \n",
        "    def init_hidden(self,input):\n",
        "        hidden = Variable(torch.zeros(self.n_layers*1, input.size(0), self.hidden_size)).cuda() if USE_CUDA else Variable(torch.zeros(self.n_layers*2,input.size(0), self.hidden_size))\n",
        "        context = Variable(torch.zeros(self.n_layers*1, input.size(0), self.hidden_size)).cuda() if USE_CUDA else Variable(torch.zeros(self.n_layers*2, input.size(0), self.hidden_size))\n",
        "        return (hidden,context)\n",
        "    \n",
        "    def forward(self, input,context,encoder_outputs,encoder_maskings,training=True):\n",
        "        embedded = self.embedding(input)\n",
        "        hidden = self.init_hidden(input)\n",
        "        decode=[]\n",
        "        aligns = encoder_outputs.transpose(0,1)\n",
        "        length = encoder_outputs.size(1)\n",
        "        for i in range(length):\n",
        "            aligned = aligns[i].unsqueeze(1)# B,1,D\n",
        "            _, hidden = self.lstm(torch.cat((embedded,context,aligned),2), hidden) \n",
        "            if i==0: \n",
        "                intent_hidden = hidden[0].clone() \n",
        "                intent_context = self.Attention(intent_hidden, encoder_outputs,encoder_maskings) \n",
        "                concated = torch.cat((intent_hidden,intent_context.transpose(0,1)),2)\n",
        "                intent_score = self.intent_out(concated.squeeze(0)) # B,D\n",
        "\n",
        "            concated = torch.cat((hidden[0],context.transpose(0,1)),2)\n",
        "            score = self.slot_out(concated.squeeze(0))\n",
        "            softmaxed = F.log_softmax(score)\n",
        "            decode.append(softmaxed)\n",
        "            _,input = torch.max(softmaxed,1)\n",
        "            embedded = self.embedding(input.unsqueeze(1))\n",
        "            context = self.Attention(hidden[0], encoder_outputs,encoder_maskings) \n",
        "        slot_scores = torch.cat(decode,1)\n",
        "        return slot_scores.view(input.size(0)*length,-1), intent_score"
      ],
      "metadata": {
        "id": "E37PLQZ6Bgih"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "Ua9ixH1ins_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE=0.001\n",
        "EMBEDDING_SIZE=64\n",
        "HIDDEN_SIZE=64\n",
        "BATCH_SIZE=16\n",
        "LENGTH=50\n",
        "STEP_SIZE=10\n",
        "encoder = Encoder(len(word2index),EMBEDDING_SIZE,HIDDEN_SIZE)\n",
        "decoder = Decoder(len(tag2index),len(intent2index),len(tag2index)//3,HIDDEN_SIZE*2)\n",
        "if USE_CUDA:\n",
        "    encoder = encoder.cuda()\n",
        "    decoder = decoder.cuda()\n",
        "    \n",
        "encoder.init_weights()\n",
        "decoder.init_weights()\n",
        "\n",
        "loss_function_1 = nn.CrossEntropyLoss(ignore_index=0)\n",
        "loss_function_2 = nn.CrossEntropyLoss()\n",
        "enc_optim= optim.Adam(encoder.parameters(), lr=LEARNING_RATE)\n",
        "dec_optim = optim.Adam(decoder.parameters(),lr=LEARNING_RATE)\n",
        "for step in range(STEP_SIZE):\n",
        "    losses=[]\n",
        "    for i, batch in enumerate(getBatch(BATCH_SIZE,train_data)):\n",
        "        x,y_1,y_2 = zip(*batch)\n",
        "        x = torch.cat(x)\n",
        "        tag_target = torch.cat(y_1)\n",
        "        intent_target = torch.cat(y_2)\n",
        "        x_mask = torch.cat([Variable(torch.ByteTensor(tuple(map(lambda s: s ==0, t.data)))).cuda() if USE_CUDA else Variable(torch.ByteTensor(tuple(map(lambda s: s ==0, t.data)))) for t in x]).view(BATCH_SIZE,-1)\n",
        "        y_1_mask = torch.cat([Variable(torch.ByteTensor(tuple(map(lambda s: s ==0, t.data)))).cuda() if USE_CUDA else Variable(torch.ByteTensor(tuple(map(lambda s: s ==0, t.data)))) for t in tag_target]).view(BATCH_SIZE,-1)\n",
        " \n",
        "        encoder.zero_grad()\n",
        "        decoder.zero_grad()\n",
        "\n",
        "        output, hidden_c = encoder(x,x_mask)\n",
        "        start_decode = Variable(torch.LongTensor([[word2index['<SOS>']]*BATCH_SIZE])).cuda().transpose(1,0) if USE_CUDA else Variable(torch.LongTensor([[word2index['<SOS>']]*BATCH_SIZE])).transpose(1,0)\n",
        "\n",
        "        tag_score, intent_score = decoder(start_decode,hidden_c,output,x_mask)\n",
        "\n",
        "        loss_1 = loss_function_1(tag_score,tag_target.view(-1))\n",
        "        loss_2 = loss_function_2(intent_score,intent_target)\n",
        "\n",
        "        loss = loss_1+loss_2\n",
        "        losses.append(loss.data.cpu().numpy() if USE_CUDA else loss.data.numpy()[0])\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm(encoder.parameters(), 5.0)\n",
        "        torch.nn.utils.clip_grad_norm(decoder.parameters(), 5.0)\n",
        "\n",
        "        enc_optim.step()\n",
        "        dec_optim.step()\n",
        "\n",
        "        if i % 100==0:\n",
        "            print(\"Step\",step,\" epoch\",i,\" : \",np.mean(losses))\n",
        "            losses=[]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeC-9I_tB40n",
        "outputId": "b7e6aaa7-a4d0-47d6-ac5f-de2717107afa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:85: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/cuda/Indexing.cu:1239.)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:87: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:122: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py:175: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/cuda/Indexing.cu:1239.)\n",
            "  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0  epoch 0  :  7.898047\n",
            "Step 0  epoch 100  :  3.6044898\n",
            "Step 0  epoch 200  :  2.6005187\n",
            "Step 1  epoch 0  :  1.9219376\n",
            "Step 1  epoch 100  :  2.076663\n",
            "Step 1  epoch 200  :  1.81909\n",
            "Step 2  epoch 0  :  2.1891344\n",
            "Step 2  epoch 100  :  1.3031497\n",
            "Step 2  epoch 200  :  1.0028843\n",
            "Step 3  epoch 0  :  0.87785685\n",
            "Step 3  epoch 100  :  0.83476037\n",
            "Step 3  epoch 200  :  0.74223405\n",
            "Step 4  epoch 0  :  0.51988137\n",
            "Step 4  epoch 100  :  0.6814476\n",
            "Step 4  epoch 200  :  0.6018121\n",
            "Step 5  epoch 0  :  0.3834911\n",
            "Step 5  epoch 100  :  0.55336744\n",
            "Step 5  epoch 200  :  0.5272619\n",
            "Step 6  epoch 0  :  0.4536983\n",
            "Step 6  epoch 100  :  0.4360807\n",
            "Step 6  epoch 200  :  0.42489213\n",
            "Step 7  epoch 0  :  0.12765232\n",
            "Step 7  epoch 100  :  0.3659593\n",
            "Step 7  epoch 200  :  0.31658638\n",
            "Step 8  epoch 0  :  0.17985414\n",
            "Step 8  epoch 100  :  0.3044225\n",
            "Step 8  epoch 200  :  0.25390166\n",
            "Step 9  epoch 0  :  0.55665755\n",
            "Step 9  epoch 100  :  0.24930415\n",
            "Step 9  epoch 200  :  0.21158278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving the models"
      ],
      "metadata": {
        "id": "tGbffN7qnzVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "model_dir = './models/'\n",
        "if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "    \n",
        "torch.save(decoder.state_dict(),os.path.join(model_dir,'decoder.pkl'))\n",
        "torch.save(encoder.state_dict(),os.path.join(model_dir, 'encoder.pkl'))\n",
        "print(\"Train Complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7DtXd32NlZN",
        "outputId": "5001a47d-223f-4f14-da7c-b149c8c54eb4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inferencing"
      ],
      "metadata": {
        "id": "oL2Y-B9eEihb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from data import *\n",
        "from model import Encoder,Decoder\n",
        "_,word2index,tag2index,intent2index = preprocessing('/content/RNN-for-Joint-NLU/atis-2.train.w-intent.iob',60)\n",
        "index2tag = {v:k for k,v in tag2index.items()}\n",
        "index2intent = {v:k for k,v in intent2index.items()}\n",
        "encoder = Encoder(len(word2index),64,64)\n",
        "decoder = Decoder(len(tag2index),len(intent2index),len(tag2index)//3,64*2)\n",
        "\n",
        "encoder.load_state_dict(torch.load('/content/RNN-for-Joint-NLU/models/encoder.pkl'))\n",
        "decoder.load_state_dict(torch.load('/content/RNN-for-Joint-NLU/models/decoder.pkl'))\n",
        "if USE_CUDA:\n",
        "    encoder = encoder.cuda()\n",
        "    decoder = decoder.cuda()\n",
        "test = open(\"/content/RNN-for-Joint-NLU/atis-2.dev.w-intent.iob\",\"r\").readlines()\n",
        "test = [t[:-1] for t in test]\n",
        "test = [[t.split(\"\\t\")[0].split(\" \"),t.split(\"\\t\")[1].split(\" \")[:-1],t.split(\"\\t\")[1].split(\" \")[-1]] for t in test]\n",
        "test = [[t[0][1:-1],t[1][1:],t[2]] for t in test]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7woZ30tB7bb",
        "outputId": "0d5e9894-9ea3-429e-816b-061cb4b1cf92"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed_data_path : /content/RNN-for-Joint-NLU/data/\n",
            "Successfully load data. # of set : 4478 \n",
            "# of vocab : 867, # of slot_tag : 120, # of intent_tag : 22\n",
            "Preprocessing complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference #1"
      ],
      "metadata": {
        "id": "SBKA2v2Xn8A2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = random.choice(range(len(test)))\n",
        "test_raw = test[index][0]\n",
        "test_in = prepare_sequence(test_raw,word2index)\n",
        "test_mask = Variable(torch.ByteTensor(tuple(map(lambda s: s ==0, test_in.data)))).cuda() if USE_CUDA else Variable(torch.ByteTensor(tuple(map(lambda s: s ==0, test_in.data)))).view(1,-1)\n",
        "start_decode = Variable(torch.LongTensor([[word2index['<SOS>']]*1])).cuda().transpose(1,0) if USE_CUDA else Variable(torch.LongTensor([[word2index['<SOS>']]*1])).transpose(1,0)\n",
        "\n",
        "output, hidden_c = encoder(test_in.unsqueeze(0),test_mask.unsqueeze(0))\n",
        "tag_score, intent_score = decoder(start_decode,hidden_c,output,test_mask)\n",
        "\n",
        "v,i = torch.max(tag_score,1)\n",
        "print(\"Input Sentence : \",*test[index][0])\n",
        "print(\"Truth        : \",*test[index][1])\n",
        "print(\"Prediction : \",*list(map(lambda ii:index2tag[ii],i.data.tolist())))\n",
        "v,i = torch.max(intent_score,1)\n",
        "print(\"Truth        : \",test[index][2])\n",
        "print(\"Prediction : \",index2intent[i.data.tolist()[0]])"
      ],
      "metadata": {
        "id": "Z2y4-t9RG55n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6345e6f9-239b-44c0-b0b1-88529c372835"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sentence :  thanks and what's the last flight back from washington to boston\n",
            "Truth        :  O O O O B-flight_mod O O O B-fromloc.city_name O B-toloc.city_name\n",
            "Prediction :  O O O O B-flight_mod O O O B-fromloc.city_name O B-toloc.city_name\n",
            "Truth        :  atis_flight\n",
            "Prediction :  atis_flight\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/RNN-for-Joint-NLU/model.py:94: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/cuda/Indexing.cu:1239.)\n",
            "  attn_energies = attn_energies.squeeze(1).masked_fill(encoder_maskings,-1e12) # PAD masking\n",
            "/content/RNN-for-Joint-NLU/model.py:96: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  alpha = F.softmax(attn_energies) # B,T\n",
            "/content/RNN-for-Joint-NLU/model.py:131: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  softmaxed = F.log_softmax(score)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference #2"
      ],
      "metadata": {
        "id": "gik4SVWPoAZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = random.choice(range(len(test)))\n",
        "test_raw = test[index][0]\n",
        "test_in = prepare_sequence(test_raw,word2index)\n",
        "test_mask = Variable(torch.ByteTensor(tuple(map(lambda s: s ==0, test_in.data)))).cuda() if USE_CUDA else Variable(torch.ByteTensor(tuple(map(lambda s: s ==0, test_in.data)))).view(1,-1)\n",
        "start_decode = Variable(torch.LongTensor([[word2index['<SOS>']]*1])).cuda().transpose(1,0) if USE_CUDA else Variable(torch.LongTensor([[word2index['<SOS>']]*1])).transpose(1,0)\n",
        "\n",
        "output, hidden_c = encoder(test_in.unsqueeze(0),test_mask.unsqueeze(0))\n",
        "tag_score, intent_score = decoder(start_decode,hidden_c,output,test_mask)\n",
        "\n",
        "v,i = torch.max(tag_score,1)\n",
        "print(\"Input Sentence : \",*test[index][0])\n",
        "print(\"Truth        : \",*test[index][1])\n",
        "print(\"Prediction : \",*list(map(lambda ii:index2tag[ii],i.data.tolist())))\n",
        "v,i = torch.max(intent_score,1)\n",
        "print(\"Truth        : \",test[index][2])\n",
        "print(\"Prediction : \",index2intent[i.data.tolist()[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fxv9e6C3TobL",
        "outputId": "0d729415-5cbd-4ab7-bd14-3cedd34c7266"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sentence :  does flight dl 1083 from philadelphia to denver fly on saturdays\n",
            "Truth        :  O O B-airline_code B-flight_number O B-fromloc.city_name O B-toloc.city_name O O B-depart_date.day_name\n",
            "Prediction :  O O B-airline_code I-airline_name O B-fromloc.city_name O B-toloc.city_name O O B-depart_date.day_name\n",
            "Truth        :  atis_flight\n",
            "Prediction :  atis_flight\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/RNN-for-Joint-NLU/model.py:94: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/cuda/Indexing.cu:1239.)\n",
            "  attn_energies = attn_energies.squeeze(1).masked_fill(encoder_maskings,-1e12) # PAD masking\n",
            "/content/RNN-for-Joint-NLU/model.py:96: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  alpha = F.softmax(attn_energies) # B,T\n",
            "/content/RNN-for-Joint-NLU/model.py:131: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  softmaxed = F.log_softmax(score)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference #3"
      ],
      "metadata": {
        "id": "F548nrOAoCa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index = random.choice(range(len(test)))\n",
        "test_raw = test[index][0]\n",
        "test_in = prepare_sequence(test_raw,word2index)\n",
        "test_mask = Variable(torch.ByteTensor(tuple(map(lambda s: s ==0, test_in.data)))).cuda() if USE_CUDA else Variable(torch.ByteTensor(tuple(map(lambda s: s ==0, test_in.data)))).view(1,-1)\n",
        "start_decode = Variable(torch.LongTensor([[word2index['<SOS>']]*1])).cuda().transpose(1,0) if USE_CUDA else Variable(torch.LongTensor([[word2index['<SOS>']]*1])).transpose(1,0)\n",
        "\n",
        "output, hidden_c = encoder(test_in.unsqueeze(0),test_mask.unsqueeze(0))\n",
        "tag_score, intent_score = decoder(start_decode,hidden_c,output,test_mask)\n",
        "\n",
        "v,i = torch.max(tag_score,1)\n",
        "print(\"Input Sentence : \",*test[index][0])\n",
        "print(\"Truth        : \",*test[index][1])\n",
        "print(\"Prediction : \",*list(map(lambda ii:index2tag[ii],i.data.tolist())))\n",
        "v,i = torch.max(intent_score,1)\n",
        "print(\"Truth        : \",test[index][2])\n",
        "print(\"Prediction : \",index2intent[i.data.tolist()[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZuSDzHjn3fA",
        "outputId": "d33e2d4d-56c3-4c50-ad55-62609835b97c"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Sentence :  show me all flights from san francisco to boston philadelphia or baltimore\n",
            "Truth        :  O O O O O B-fromloc.city_name I-fromloc.city_name O B-toloc.city_name B-toloc.city_name O B-toloc.city_name\n",
            "Prediction :  O O O O O B-fromloc.city_name I-fromloc.city_name O B-toloc.city_name B-toloc.city_name I-toloc.city_name B-toloc.city_name\n",
            "Truth        :  atis_flight\n",
            "Prediction :  atis_flight\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/RNN-for-Joint-NLU/model.py:94: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  ../aten/src/ATen/native/cuda/Indexing.cu:1239.)\n",
            "  attn_energies = attn_energies.squeeze(1).masked_fill(encoder_maskings,-1e12) # PAD masking\n",
            "/content/RNN-for-Joint-NLU/model.py:96: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  alpha = F.softmax(attn_energies) # B,T\n",
            "/content/RNN-for-Joint-NLU/model.py:131: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  softmaxed = F.log_softmax(score)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training SVM"
      ],
      "metadata": {
        "id": "hvfN8OykV7Sz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading spacy"
      ],
      "metadata": {
        "id": "H6G16mpMoFy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l62mn-uAWYs_",
        "outputId": "9cb82489-5e40-4f69-f802-fad6ba7b4fb8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-md==3.3.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.3.0/en_core_web_md-3.3.0-py3-none-any.whl (33.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 33.5 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-md==3.3.0) (3.3.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (57.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (21.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.6.2)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.4.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.21.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.4.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.0.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (4.64.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.8.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.9.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.23.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.0.7)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (4.1.1)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (0.7.8)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (8.0.17)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.0.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.11.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.3.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (5.2.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2022.6.15)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-md==3.3.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-3.3.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")\n",
        "print(\"Number of vectors: {}\".format(nlp.vocab.vectors_length))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsGVfRu2VwIm",
        "outputId": "34e46c85-f2b6-4bd8-ee0c-ccb4aa5337eb"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of vectors: 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation"
      ],
      "metadata": {
        "id": "7-BiQW1WoISH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_train = pd.DataFrame(train_copy, columns = ['text', 'slots', 'label'])\n",
        "df_train['text'] = [\" \".join(i) for i in df_train['text'].values]\n",
        "df_train['slots'] = [\" \".join(i) for i in df_train['slots'].values]\n",
        "df_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1036
        },
        "id": "WZBuXPGUbfQK",
        "outputId": "7a84f4ac-af1b-4394-8f58-072510c0249a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                   text  \\\n",
              "0     i want to fly from baltimore to dallas round trip   \n",
              "1     round trip fares from baltimore to philadelphi...   \n",
              "2     show me the flights arriving on baltimore on j...   \n",
              "3     what are the flights which depart from san fra...   \n",
              "4     which airlines fly from boston to washington d...   \n",
              "...                                                 ...   \n",
              "4473  what's the first flight after 1 pm leaving was...   \n",
              "4474  what are the nonstop flights on america west o...   \n",
              "4475  tell me about ground transportation between or...   \n",
              "4476  i'd like a twa flight from las vegas to new yo...   \n",
              "4477  is there a delta flight from denver to san fra...   \n",
              "\n",
              "                                                  slots                label  \n",
              "0     O O O O O B-fromloc.city_name O B-toloc.city_n...          atis_flight  \n",
              "1     B-round_trip I-round_trip O O B-fromloc.city_n...         atis_airfare  \n",
              "2     O O O O O O B-toloc.city_name O B-arrive_date....          atis_flight  \n",
              "3     O O O O O O O B-fromloc.city_name I-fromloc.ci...          atis_flight  \n",
              "4     O O O O B-fromloc.city_name O B-toloc.city_nam...         atis_airline  \n",
              "...                                                 ...                  ...  \n",
              "4473  O O B-flight_mod O B-depart_time.time_relative...          atis_flight  \n",
              "4474  O O O B-flight_stop O O B-airline_name I-airli...          atis_flight  \n",
              "4475  O O O O O O B-fromloc.airport_name I-fromloc.a...  atis_ground_service  \n",
              "4476  O O O B-airline_code O O B-fromloc.city_name I...          atis_flight  \n",
              "4477  O O O B-airline_name O O B-fromloc.city_name O...           atis_fligh  \n",
              "\n",
              "[4478 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-555d09d1-9c37-4d06-b921-3f2ca6fb6627\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>slots</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i want to fly from baltimore to dallas round trip</td>\n",
              "      <td>O O O O O B-fromloc.city_name O B-toloc.city_n...</td>\n",
              "      <td>atis_flight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>round trip fares from baltimore to philadelphi...</td>\n",
              "      <td>B-round_trip I-round_trip O O B-fromloc.city_n...</td>\n",
              "      <td>atis_airfare</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>show me the flights arriving on baltimore on j...</td>\n",
              "      <td>O O O O O O B-toloc.city_name O B-arrive_date....</td>\n",
              "      <td>atis_flight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>what are the flights which depart from san fra...</td>\n",
              "      <td>O O O O O O O B-fromloc.city_name I-fromloc.ci...</td>\n",
              "      <td>atis_flight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>which airlines fly from boston to washington d...</td>\n",
              "      <td>O O O O B-fromloc.city_name O B-toloc.city_nam...</td>\n",
              "      <td>atis_airline</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4473</th>\n",
              "      <td>what's the first flight after 1 pm leaving was...</td>\n",
              "      <td>O O B-flight_mod O B-depart_time.time_relative...</td>\n",
              "      <td>atis_flight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4474</th>\n",
              "      <td>what are the nonstop flights on america west o...</td>\n",
              "      <td>O O O B-flight_stop O O B-airline_name I-airli...</td>\n",
              "      <td>atis_flight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4475</th>\n",
              "      <td>tell me about ground transportation between or...</td>\n",
              "      <td>O O O O O O B-fromloc.airport_name I-fromloc.a...</td>\n",
              "      <td>atis_ground_service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4476</th>\n",
              "      <td>i'd like a twa flight from las vegas to new yo...</td>\n",
              "      <td>O O O B-airline_code O O B-fromloc.city_name I...</td>\n",
              "      <td>atis_flight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4477</th>\n",
              "      <td>is there a delta flight from denver to san fra...</td>\n",
              "      <td>O O O B-airline_name O O B-fromloc.city_name O...</td>\n",
              "      <td>atis_fligh</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4478 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-555d09d1-9c37-4d06-b921-3f2ca6fb6627')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-555d09d1-9c37-4d06-b921-3f2ca6fb6627 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-555d09d1-9c37-4d06-b921-3f2ca6fb6627');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_test = pd.DataFrame(test, columns = ['text', 'slots', 'label'])\n",
        "df_test['text'] = [\" \".join(i) for i in df_test['text'].values]\n",
        "df_test['slots'] = [\" \".join(i) for i in df_test['slots'].values]\n",
        "df_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1052
        },
        "id": "_S-Qm5YfY1Rd",
        "outputId": "c54cc454-30d8-44e9-b750-3c9271544f3b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  text  \\\n",
              "0    i want to fly from boston at 838 am and arrive...   \n",
              "1    show me all round trip flights between houston...   \n",
              "2    i would like some information on a flight from...   \n",
              "3    what are the coach flights between dallas and ...   \n",
              "4               i'm flying from boston to the bay area   \n",
              "..                                                 ...   \n",
              "495                       pm flights dallas to atlanta   \n",
              "496  information on flights from baltimore to phila...   \n",
              "497  what flights from atlanta to st. louis on tues...   \n",
              "498     show me ground transportation in san francisco   \n",
              "499  what flights do you have from newark new jerse...   \n",
              "\n",
              "                                                 slots                label  \n",
              "0    O O O O O B-fromloc.city_name O B-depart_time....          atis_flight  \n",
              "1    O O O B-round_trip I-round_trip O O B-fromloc....          atis_flight  \n",
              "2    O O O O O O O O O B-fromloc.city_name O B-tolo...          atis_flight  \n",
              "3    O O O B-class_type O O B-fromloc.city_name O B...          atis_flight  \n",
              "4    O O O B-fromloc.city_name O O B-toloc.city_nam...          atis_flight  \n",
              "..                                                 ...                  ...  \n",
              "495  B-depart_time.period_of_day O B-fromloc.city_n...          atis_flight  \n",
              "496    O O O O B-fromloc.city_name O B-toloc.city_name          atis_flight  \n",
              "497  O O O B-fromloc.city_name O B-toloc.city_name ...          atis_flight  \n",
              "498                  O O O O O B-city_name I-city_name  atis_ground_service  \n",
              "499  O O O O O O B-fromloc.city_name B-fromloc.stat...           atis_fligh  \n",
              "\n",
              "[500 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-74fb9ed2-2831-43d4-b719-2987b1869c7c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>slots</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>i want to fly from boston at 838 am and arrive...</td>\n",
              "      <td>O O O O O B-fromloc.city_name O B-depart_time....</td>\n",
              "      <td>atis_flight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>show me all round trip flights between houston...</td>\n",
              "      <td>O O O B-round_trip I-round_trip O O B-fromloc....</td>\n",
              "      <td>atis_flight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i would like some information on a flight from...</td>\n",
              "      <td>O O O O O O O O O B-fromloc.city_name O B-tolo...</td>\n",
              "      <td>atis_flight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>what are the coach flights between dallas and ...</td>\n",
              "      <td>O O O B-class_type O O B-fromloc.city_name O B...</td>\n",
              "      <td>atis_flight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i'm flying from boston to the bay area</td>\n",
              "      <td>O O O B-fromloc.city_name O O B-toloc.city_nam...</td>\n",
              "      <td>atis_flight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>pm flights dallas to atlanta</td>\n",
              "      <td>B-depart_time.period_of_day O B-fromloc.city_n...</td>\n",
              "      <td>atis_flight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>information on flights from baltimore to phila...</td>\n",
              "      <td>O O O O B-fromloc.city_name O B-toloc.city_name</td>\n",
              "      <td>atis_flight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>what flights from atlanta to st. louis on tues...</td>\n",
              "      <td>O O O B-fromloc.city_name O B-toloc.city_name ...</td>\n",
              "      <td>atis_flight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>show me ground transportation in san francisco</td>\n",
              "      <td>O O O O O B-city_name I-city_name</td>\n",
              "      <td>atis_ground_service</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>what flights do you have from newark new jerse...</td>\n",
              "      <td>O O O O O O B-fromloc.city_name B-fromloc.stat...</td>\n",
              "      <td>atis_fligh</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-74fb9ed2-2831-43d4-b719-2987b1869c7c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-74fb9ed2-2831-43d4-b719-2987b1869c7c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-74fb9ed2-2831-43d4-b719-2987b1869c7c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sen_train = df_train['text'].tolist()\n",
        "labels_train = df_train['label'].tolist()\n",
        "sen_test = df_test['text'].tolist()\n",
        "labels_test = df_test['label'].tolist()\n",
        "print(len(sen_train),len(labels_train),len(sen_test),len(labels_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBZbZ3boY1Oy",
        "outputId": "8ba36018-a5c7-4d23-fb10-6c0ae20877eb"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4478 4478 500 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding the sentences"
      ],
      "metadata": {
        "id": "cmqXWYpNoOre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_sentences(sentences):\n",
        "    n_sentences = len(sentences)\n",
        "    X = np.zeros((n_sentences, 300))\n",
        "    for idx, sentence in enumerate(sentences):\n",
        "        doc = nlp(sentence)\n",
        "        X[idx, :] = doc.vector\n",
        "    return X\n",
        "\n",
        "train_X = encode_sentences(sen_train)\n",
        "test_X = encode_sentences(sen_test)"
      ],
      "metadata": {
        "id": "NI_3i1DAY1MJ"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "labels_test = le.fit_transform(labels_test)\n",
        "labels_train = le.fit_transform(labels_train)"
      ],
      "metadata": {
        "id": "ILqOz0tsY1CA"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max(labels_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_u1YVtQ6j8hN",
        "outputId": "1573171e-98ff-4499-a029-b0e803b51137"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "clf = SVC(C=1)\n",
        "clf.fit(train_X, labels_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNOaOfgZjmnk",
        "outputId": "e6053ccc-e606-4495-82be-7e27e6506697"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_clf(X,y):\n",
        "    y_pred = clf.predict(X)\n",
        "    n_correct = 0\n",
        "    for i in range(len(y)):\n",
        "        if y_pred[i] == y[i]:\n",
        "            n_correct += 1\n",
        "\n",
        "    print(\"Predicted {} correctly out of {}\".format(n_correct, len(y)))\n",
        "    print(\"Model accuracy: {}%\".format(round(n_correct/len(y)*100),2))\n",
        "    \n",
        "print('Validation on the train set results:')\n",
        "validate_clf(train_X, labels_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkc8PJu3jrzm",
        "outputId": "2400228d-5f20-476c-d3b6-508b6be08a36"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation on the train set results:\n",
            "Predicted 4063 correctly out of 4478\n",
            "Model accuracy: 91%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print('Validation on the test set results:')\n",
        "validate_clf(test_X, labels_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGlpE4QajrxS",
        "outputId": "470b32c8-a154-4361-cbf9-418e5468c40b"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation on the test set results:\n",
            "Predicted 44 correctly out of 500\n",
            "Model accuracy: 9%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q: Why is accuracy so low? <br>\n",
        "A: `Here`."
      ],
      "metadata": {
        "id": "xd1OXOgHofHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "waaPvAq9jrus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fTr_MM0cjrPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### References:\n",
        "1. [Dataset](https://github.com/yvchen/JointSLU/tree/master/data)\n",
        "2. [Attention-Based Recurrent Neural Network Models for Joint Intent Detection and Slot Filling](https://arxiv.org/pdf/1609.01454.pdf)\n",
        "3. [Github - RNN-for-Joint-NLU](https://github.com/DSKSD/RNN-for-Joint-NLU)\n",
        "4. [Intent Classification with SVM](https://www.kaggle.com/code/oleksandrarsentiev/intent-classification-with-svm)\n",
        "5. [Airline-Travel-Information-System-ATIS-Text-Analysis](https://github.com/nawaz-kmr/Airline-Travel-Information-System-ATIS-Text-Analysis#airline-travel-information-system-atis-text-analysis)"
      ],
      "metadata": {
        "id": "EQ-HpnA1TzgK"
      }
    }
  ]
}